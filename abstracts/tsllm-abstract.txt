Analyzing and predicting time series data has great importance across many fields of science, engineering, and medicine. Countless models have been developed for tackling specific tasks on specific datasets, but these require training data and extensive experimentation for every application. Recently foundation models, specifically large language models (LLMs), have demonstrated their potential to outperform narrow models by utilizing transfer learning and contextual information. However, LLMs struggle to handle numerical time series data due to inefficient encodings and little relevant training. In this work, we address this limitation by integrating an LLM with a flexible patch-based encoder-decoder model for time series signals, enabling it to use interleaved text and numerical time series inputs and generate interleaved outputs. We train this Time Series augmented LLM (TsLLM) on a large corpus of time series analysis and prediction tasks spanning forecasting, report generation, question-answering, classification, and more, all unified within the framework of next token prediction. The flexibility of TsLLM allows it to use contextual information encoded as text and few-shot in-context learning to adapt and perform well on unseen data. Empirical results show competitive performance between TsLLM and task-specific specialized models on a range of benchmark datasets, demonstrating the clear value of LLM-based foundation models for solving time series tasks.